// AI-Powered Mandarin Photo Captions - No Static Data Required!

// API configuration - Multiple API keys for redundancy
const OPENROUTER_API_KEYS = [
    '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',
    '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'
];
const OPENROUTER_API_URL = 'https://openrouter.ai/api/v1/chat/completions';

// Current API key index (for round-robin or fallback)
let currentApiKeyIndex = 0;

// Function to get current API key
function getCurrentApiKey() {
    return OPENROUTER_API_KEYS[currentApiKeyIndex];
}

// Function to switch to next API key (for fallback)
function switchToNextApiKey() {
    currentApiKeyIndex = (currentApiKeyIndex + 1) % OPENROUTER_API_KEYS.length;
    console.log(`üîÑ Switched to API key ${currentApiKeyIndex + 1} of ${OPENROUTER_API_KEYS.length}`);
}

// Function to make API request with automatic fallback
async function makeApiRequestWithFallback(requestBody, maxRetries = OPENROUTER_API_KEYS.length) {
    let lastError = null;
    
    for (let attempt = 0; attempt < maxRetries; attempt++) {
        try {
            const currentKey = getCurrentApiKey();
            console.log(`üîë Using API key ${currentApiKeyIndex + 1} (attempt ${attempt + 1}/${maxRetries})`);
            
            const response = await fetch(OPENROUTER_API_URL, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${currentKey}`,
                    'Content-Type': 'application/json',
                    'HTTP-Referer': window.location.origin,
                    'X-Title': 'Mandarin Photo Captions'
                },
                body: JSON.stringify(requestBody)
            });
            
            if (response.ok) {
                console.log(`‚úÖ API request successful with key ${currentApiKeyIndex + 1}`);
                return await response.json();
            } else {
                const errorText = await response.text();
                console.warn(`‚ö†Ô∏è API key ${currentApiKeyIndex + 1} failed: ${response.status} - ${errorText}`);
                lastError = new Error(`API request failed: ${response.status} ${response.statusText} - ${errorText}`);
                
                // Switch to next API key for next attempt
                if (attempt < maxRetries - 1) {
                    switchToNextApiKey();
                }
            }
        } catch (error) {
            console.warn(`‚ö†Ô∏è API key ${currentApiKeyIndex + 1} error:`, error);
            lastError = error;
            
            // Switch to next API key for next attempt
            if (attempt < maxRetries - 1) {
                switchToNextApiKey();
            }
        }
    }
    
    // All API keys failed
    throw lastError || new Error('All API keys failed');
}

// Image analysis validation
function validateImageAnalysis(analysis) {
    const issues = [];
    
    // Check required fields
    if (!analysis.mainSubject || analysis.mainSubject.length < 2) {
        issues.push('mainSubject is missing or too short');
    }
    
    if (!analysis.category || !['food', 'animals', 'objects', 'places', 'nature', 'activities', 'people', 'text', 'art'].includes(analysis.category)) {
        issues.push('category is missing or invalid');
    }
    
    if (!analysis.description || analysis.description.length < 10) {
        issues.push('description is missing or too short');
    }
    
    // Check for generic/unhelpful descriptions
    const genericTerms = ['photo', 'image', 'picture', 'something', 'object', 'thing'];
    if (genericTerms.some(term => analysis.mainSubject.toLowerCase().includes(term))) {
        issues.push('mainSubject is too generic');
    }
    
    if (genericTerms.some(term => analysis.description.toLowerCase().includes(term))) {
        issues.push('description is too generic');
    }
    
    // Check confidence level
    if (analysis.confidence === 'low') {
        issues.push('low confidence in analysis');
    }
    
    // Check for AI-generated keywords (optional but preferred)
    if (!analysis.keywords || !Array.isArray(analysis.keywords) || analysis.keywords.length === 0) {
        issues.push('AI-generated keywords are missing - this may reduce caption quality');
    }
    
    return {
        isValid: issues.length === 0,
        issues: issues
    };
}

// Caption validation function
function validateCaptions(captions, analysis) {
    const validatedCaptions = [];
    
    for (const caption of captions) {
        // Check if caption has all required fields
        if (!caption.chinese || !caption.pinyin || !caption.english) {
            console.warn('Caption missing required fields:', caption);
            continue;
        }
        
        // Check for relevance if provided
        if (caption.relevance === 'low') {
            console.warn('Low relevance caption filtered out:', caption.chinese);
            continue;
        }
        
        // Check for generic/unrelated content
        const genericPhrases = [
            '√®¬ø‚Ñ¢√¶Àú¬Ø√§¬∏‚Ç¨√§¬∏¬™', '√®¬ø‚Ñ¢√¶Àú¬Ø√ß‚Ä¶¬ß√ß‚Ä∞‚Ä°', '√®¬ø‚Ñ¢√¶Àú¬Ø√•‚Ä∫¬æ√ß‚Ä∞‚Ä°', '√®¬ø‚Ñ¢√¶Àú¬Ø√§¬∏≈ì√®¬•¬ø', '√®¬ø‚Ñ¢√¶Àú¬Ø√ß‚Ä∞¬©√§¬Ω‚Äú',
            'this is a', 'this is an', 'this is the', 'this looks like'
        ];
        
        const isGeneric = genericPhrases.some(phrase => 
            caption.chinese.toLowerCase().includes(phrase) || 
            caption.english.toLowerCase().includes(phrase)
        );
        
        if (isGeneric) {
            console.warn('Generic caption filtered out:', caption.chinese);
            continue;
        }
        
        // Check if caption relates to the analysis using AI-generated keywords
        const subjectWords = analysis.mainSubject.toLowerCase().split(' ');
        const categoryWords = analysis.category.toLowerCase().split(' ');
        const descriptionWords = analysis.description.toLowerCase().split(' ');
        const aiKeywords = analysis.keywords ? analysis.keywords.map(k => k.toLowerCase()) : [];
        const chineseKeywords = analysis.chineseKeywords ? analysis.chineseKeywords : [];
        
        const allRelevantWords = [...subjectWords, ...categoryWords, ...descriptionWords, ...aiKeywords];
        
        // Enhanced relevance check using AI-generated keywords
        const captionText = (caption.chinese + ' ' + caption.english).toLowerCase();
        const hasRelevantWords = allRelevantWords.some(word => 
            word.length > 2 && captionText.includes(word)
        );
        
        // Check for Chinese keyword relevance
        const hasChineseKeywords = chineseKeywords.some(keyword => 
            caption.chinese.includes(keyword)
        );
        
        if (!hasRelevantWords && !hasChineseKeywords && analysis.confidence !== 'low') {
            console.warn('Caption may not be relevant to image:', caption.chinese);
            // Don't filter out completely, but mark for review
        }
        
        validatedCaptions.push(caption);
    }
    
    // If we filtered out too many captions, return what we have
    // No fallbacks - we want personalized captions, not generic ones
    if (validatedCaptions.length < 2) {
        console.log('Warning: Only', validatedCaptions.length, 'valid captions after filtering');
    }
    
    return validatedCaptions.slice(0, 3); // Return max 3 captions
}

// Enhanced retry analysis with different approach
async function analyzeImageWithRetry(base64Image, imageFile) {
    try {
        console.log('üîÑ Attempting retry with more specific prompt...');
        
        const retryPrompt = `Look at this image very carefully. I need you to identify the most specific and obvious elements with intelligent keywords.

        Focus on:
        1. What is the most prominent thing in this image?
        2. What specific type of object, animal, food, or scene is it?
        3. What are the key visual details?
        4. Generate relevant keywords for caption generation
        
        Be very specific. Instead of "food", say "pizza" or "sandwich". Instead of "animal", say "cat" or "dog".
        
        Return in JSON format:
        {
            "mainSubject": "very specific subject name",
            "category": "food|animals|objects|places|nature|activities|people|text|art",
            "description": "specific description of what you see",
            "context": "where or when this might be",
            "mood": "the feeling",
            "colors": "main colors",
            "details": "specific visual details",
            "confidence": "high|medium|low",
            "keywords": ["specific", "keywords", "for", "this", "image"],
            "chineseKeywords": ["√§¬∏¬≠√¶‚Äì‚Ä°√•‚Ä¶¬≥√©‚Äù¬Æ√®¬Ø¬ç", "for", "better", "captions"]
        }`;

        const requestBody = {
            model: 'openai/gpt-4o-mini',
            messages: [
                {
                    role: 'user',
                    content: [
                        {
                            type: 'text',
                            text: retryPrompt
                        },
                        {
                            type: 'image_url',
                            image_url: {
                                url: `data:image/jpeg;base64,${base64Image}`
                            }
                        }
                    ]
                }
            ],
            max_tokens: 300,
            temperature: 0.1 // Lower temperature for more focused responses
        };

        const data = await makeApiRequestWithFallback(requestBody);
        const analysisText = data.choices[0].message.content;
        
        const analysis = JSON.parse(analysisText);
        console.log('‚úÖ Retry analysis completed:', analysis);
        return analysis;
        
    } catch (error) {
        console.error('Retry analysis failed:', error);
        throw new Error('AI analysis failed - unable to generate personalized captions. Please try again.');
    }
}

// API key validation
function validateApiKeys() {
    const issues = [];
    
    if (!OPENROUTER_API_KEYS || OPENROUTER_API_KEYS.length === 0) {
        issues.push('No API keys configured');
    } else {
        OPENROUTER_API_KEYS.forEach((key, index) => {
            if (!key || key.length < 10) {
                issues.push(`API key ${index + 1} is missing or invalid`);
            }
        });
    }
    
    if (issues.length > 0) {
        console.warn('API Key Issues:', issues);
        return false;
    }
    
    console.log(`‚úÖ ${OPENROUTER_API_KEYS.length} API keys configured and appear valid`);
    return true;
}

// Test API connectivity
async function testApiConnectivity() {
    try {
        console.log('Testing API connectivity with multiple keys...');
        const testRequest = {
            model: 'openai/gpt-4o-mini', // Test with reliable vision model
            messages: [{ role: 'user', content: 'Hello, can you see images?' }],
            max_tokens: 10
        };
        
        const response = await makeApiRequestWithFallback(testRequest);
        
        // makeApiRequestWithFallback returns the parsed JSON response, not the fetch response
        if (response && response.choices && response.choices[0]) {
            console.log('‚úÖ API connectivity test passed - vision model available');
            return true;
        } else {
            const errorText = await response.text();
            console.error('√¢¬ù≈í API connectivity test failed:', response.status, errorText);
            
            // No fallback model testing - we want personalized results only
            console.error('‚ùå Primary model failed - no fallback available');
            const fallbackResponse = await fetch(OPENROUTER_API_URL, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${getCurrentApiKey()}`,
                    'Content-Type': 'application/json',
                    'HTTP-Referer': window.location.origin,
                    'X-Title': 'Mandarin Photo Captions'
                },
                body: JSON.stringify({
                    model: 'google/gemini-pro-vision:free',
                    messages: [{ role: 'user', content: 'Hello' }],
                    max_tokens: 10
                })
            });
            
            if (fallbackResponse.ok) {
                // No fallback model available
                return false;
            } else {
                console.error('√¢¬ù≈í Both models failed');
                return false;
            }
        }
    } catch (error) {
        console.error('√¢¬ù≈í API connectivity test error:', error);
        return false;
    }
}

// Real image analysis using vision-capable AI model
async function analyzeImage(imageFile) {
    try {
        console.log('√∞≈∏‚Äù¬ç Analyzing actual image content...');
        
        // Convert image to base64 for API
        const base64Image = await fileToBase64(imageFile);
        
        // Enhanced prompt for more accurate image analysis with AI-generated keywords
        const prompt = `You are an expert image analyst and keyword generator. Analyze this image carefully and provide a detailed, accurate description with intelligent keywords.

        IMPORTANT INSTRUCTIONS:
        - Look at the image carefully and describe ONLY what you actually see
        - Be specific about objects, people, animals, food, or scenes
        - Identify the most prominent elements first
        - Consider the context and setting
        - Note colors, lighting, and mood
        - If you see text, describe what it says
        - If you see faces, note expressions and approximate age
        - If you see food, describe the specific dishes or ingredients
        - If you see animals, identify the species if possible
        - Generate relevant keywords that would help create accurate Mandarin captions
        - Be factual and avoid assumptions
        
        Return your analysis in this exact JSON format:
        {
            "mainSubject": "the most prominent object, person, animal, or scene in the image",
            "category": "food|animals|objects|places|nature|activities|people|text|art",
            "description": "detailed, factual description of what is visible in the image",
            "context": "the setting, location, or situation shown",
            "mood": "the feeling or atmosphere conveyed by the image",
            "colors": "dominant colors and color scheme",
            "details": "specific visual details that would help create accurate captions",
            "confidence": "high|medium|low - your confidence in the analysis",
            "alternativeSubjects": ["other notable subjects in the image"],
            "keywords": ["relevant", "keywords", "for", "caption", "generation"],
            "chineseKeywords": ["√§¬∏¬≠√¶‚Äì‚Ä°√•‚Ä¶¬≥√©‚Äù¬Æ√®¬Ø¬ç", "for", "better", "captions"]
        }
        
        Be precise and only describe what you can clearly see. Generate keywords that are specific to the image content.`;

        const requestBody = {
            model: 'openai/gpt-4o-mini', // More reliable vision model
            messages: [
                {
                    role: 'user',
                    content: [
                        {
                            type: 'text',
                            text: prompt
                        },
                        {
                            type: 'image_url',
                            image_url: {
                                url: `data:image/jpeg;base64,${base64Image}`
                            }
                        }
                    ]
                }
            ],
            max_tokens: 400,
            temperature: 0.1, // Lower temperature for more consistent, accurate analysis
            top_p: 0.9,
            frequency_penalty: 0.1,
            presence_penalty: 0.1
        };

        const data = await makeApiRequestWithFallback(requestBody);

        // Error handling is now done by makeApiRequestWithFallback

        // Data already available from makeApiRequestWithFallback
        const analysisText = data.choices[0].message.content;
        console.log('ü§î Raw AI response:', analysisText);
        
        // Parse and validate the JSON response
        let analysis;
        try {
            analysis = JSON.parse(analysisText);
            
            // Validate the analysis quality
            const validationResult = validateImageAnalysis(analysis);
            if (!validationResult.isValid) {
                console.warn('√¢¬ù≈í Analysis validation failed:', validationResult.issues);
                
                // Try to retry with a different approach if confidence is low
                if (analysis.confidence === 'low' || validationResult.issues.length > 2) {
                    console.log('√∞≈∏‚Äù‚Äû Retrying with enhanced prompt due to low quality analysis...');
                    return await analyzeImageWithRetry(base64Image, imageFile);
                }
            }
            
            console.log('‚úÖ Successfully analyzed image content:', analysis);
            return analysis;
        } catch (parseError) {
            console.warn('√¢¬ù≈í Failed to parse AI response as JSON:', parseError);
            console.log('Raw response that failed to parse:', analysisText);
            throw new Error('AI analysis failed - unable to generate personalized captions. Please try again.');
        }

    } catch (error) {
        console.error('Error analyzing image:', error);
        // No fallback analysis - we want personalized results only
        throw new Error('AI analysis failed - unable to generate personalized captions. Please try again.');
    }
}

// No fallback vision analysis - we want personalized results only
// Removed fallback function to maintain personalization
// Removed fallback function to maintain personalization
// Function removed - no fallback analysis
async function removedFallbackFunction() {
    try {
        const prompt = `Look at this image and describe what you see with keywords in JSON format:
        {
            "mainSubject": "what is the main subject",
            "category": "food|animals|objects|places|nature|activities|people",
            "description": "what the image shows",
            "context": "setting or situation",
            "mood": "feeling or atmosphere",
            "keywords": ["relevant", "keywords", "for", "caption", "generation"],
            "chineseKeywords": ["√§¬∏¬≠√¶‚Äì‚Ä°√•‚Ä¶¬≥√©‚Äù¬Æ√®¬Ø¬ç", "for", "better", "captions"]
        }`;

        const requestBody = {
            model: 'google/gemini-pro-vision:free', // Alternative vision model
            messages: [
                {
                    role: 'user',
                    content: [
                        {
                            type: 'text',
                            text: prompt
                        },
                        {
                            type: 'image_url',
                            image_url: {
                                url: `data:image/jpeg;base64,${base64Image}`
                            }
                        }
                    ]
                }
            ],
            max_tokens: 200,
            temperature: 0.3
        };

        const data = await makeApiRequestWithFallback(requestBody);
        const analysisText = data.choices[0].message.content;
        
        try {
            return JSON.parse(analysisText);
        } catch (parseError) {
            throw new Error('Failed to parse fallback response');
        }

    } catch (error) {
        console.error('Fallback vision analysis failed:', error);
        throw error;
    }
}

// No fallback analysis - we want personalized results only
// If AI analysis fails, we should throw an error to encourage retry

// Generate Chinese descriptions using AI API
async function generateChineseDescriptions(analysis) {
    try {
        // Enhanced prompt with AI-generated keywords and better context
        const prompt = `You are a native Chinese speaker creating captions for this specific image.

        IMAGE ANALYSIS:
        - Main Subject: ${analysis.mainSubject}
        - Category: ${analysis.category}
        - Description: ${analysis.description}
        - Context: ${analysis.context}
        - Mood: ${analysis.mood}
        ${analysis.colors ? `- Colors: ${analysis.colors}` : ''}
        ${analysis.details ? `- Specific Details: ${analysis.details}` : ''}
        ${analysis.confidence ? `- Analysis Confidence: ${analysis.confidence}` : ''}
        ${analysis.keywords ? `- AI-Generated Keywords: ${analysis.keywords.join(', ')}` : ''}
        ${analysis.chineseKeywords ? `- Chinese Keywords: ${analysis.chineseKeywords.join(', ')}` : ''}
        ${analysis.alternativeSubjects ? `- Alternative Subjects: ${analysis.alternativeSubjects.join(', ')}` : ''}
        
        CRITICAL REQUIREMENTS:
        1. Create captions that DIRECTLY relate to what is shown in the image
        2. Use specific vocabulary that matches the actual content and AI-generated keywords
        3. If it's food, use food-related vocabulary from the keywords
        4. If it's an animal, use animal-specific terms from the analysis
        5. If it's a place, use location-appropriate language
        6. Make sentences sound natural and conversational
        7. Include accurate pinyin with tone marks (√Ñ¬Å √É¬° √á≈Ω √É )
        8. Provide clear, accurate English translations
        9. Use varied sentence structures but keep them relevant
        10. Avoid generic phrases that could apply to any image
        11. Incorporate the AI-generated keywords naturally into the captions
        
        SENTENCE PATTERNS TO USE:
        - Direct description: "√®¬ø‚Ñ¢√¶Àú¬Ø..." (This is...)
        - Personal reaction: "√¶ÀÜ‚Äò√®¬ß‚Ä∞√•¬æ‚Äî..." (I think...)
        - Observation: "√ß≈ì‚Äπ√®¬µ¬∑√¶¬ù¬•..." (It looks...)
        - Experience: "√¶ÀÜ‚Äò√•‚Äì≈ì√¶¬¨¬¢..." (I like...)
        - Quality assessment: "√•¬æÀÜ..." (very...)
        - Keyword integration: Use the specific keywords in natural sentences
        
        Return in this exact JSON format:
        {
            "captions": [
                {
                    "chinese": "Chinese sentence that specifically relates to the image content and uses relevant keywords",
                    "pinyin": "pinyin pronunciation with tone marks",
                    "english": "Accurate English translation",
                    "relevance": "high|medium|low - how well this caption matches the image",
                    "keywordsUsed": ["keywords", "from", "analysis", "used", "in", "this", "caption"]
                }
            ]
        }`;

        const requestBody = {
            model: 'qwen/qwen-2.5-72b-instruct:free',
            messages: [
                {
                    role: 'user',
                    content: prompt
                }
            ],
            max_tokens: 800,
            temperature: 0.5, // Balanced creativity and consistency
            top_p: 0.9,
            frequency_penalty: 0.2, // Encourage varied vocabulary
            presence_penalty: 0.1
        };

        const data = await makeApiRequestWithFallback(requestBody);
        const responseText = data.choices[0].message.content;
        
        // Parse the JSON response - handle both direct JSON and markdown code blocks
        let result;
        try {
            // Try to extract JSON from markdown code blocks if present
            let jsonText = responseText;
            if (responseText.includes('```json')) {
                jsonText = responseText.split('```json')[1].split('```')[0].trim();
            } else if (responseText.includes('```')) {
                jsonText = responseText.split('```')[1].split('```')[0].trim();
            }
            
            result = JSON.parse(jsonText);
            const captions = result.captions || [];
            
            // Validate and filter captions for relevance
            const validatedCaptions = validateCaptions(captions, analysis);
            console.log('‚úÖ Caption validation completed:', validatedCaptions);
            return validatedCaptions;
        } catch (parseError) {
            console.warn('Failed to parse AI description response as JSON');
            console.log('Response text:', responseText);
            throw new Error('AI response parsing failed - unable to generate personalized captions');
        }

    } catch (error) {
        console.error('Error generating Chinese descriptions:', error);
        throw error; // Re-throw to let caller handle appropriately
    }
}


// Helper function to convert file to base64
function fileToBase64(file) {
    return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.readAsDataURL(file);
        reader.onload = () => {
            const base64 = reader.result.split(',')[1]; // Remove data:image/...;base64, prefix
            resolve(base64);
        };
        reader.onerror = error => reject(error);
    });
}

// Generate captions based on detected content
async function generateCaptions(imageAnalysis) {
    try {
        console.log('Generating AI-powered Chinese captions...');
        const captions = await generateChineseDescriptions(imageAnalysis);
        
        if (captions && captions.length > 0) {
            console.log('‚úÖ Successfully generated AI captions');
            return captions.slice(0, 3); // Return up to 3 captions
        }
        
        // If AI returns empty, throw error - we want personalized captions
        console.log('AI returned empty captions');
        throw new Error('AI failed to generate personalized captions');
        
    } catch (error) {
        console.error('Error in generateCaptions:', error);
        throw error; // Re-throw to let caller handle appropriately
    }
}

// Display captions with animation
function displayCaptions(captions) {
    const container = document.getElementById('captionsContainer');
    container.innerHTML = '';
    
    captions.forEach((caption, index) => {
        setTimeout(() => {
            const card = document.createElement('div');
            card.className = 'caption-card';
            card.innerHTML = `
                <div class="chinese-text">
                    ${caption.chinese}
                    <button class="audio-btn" onclick="speakChinese('${caption.chinese}')" title="Play audio">
                        üîä
                    </button>
                </div>
                <div class="pinyin-text">${caption.pinyin}</div>
                <div class="english-text">${caption.english}</div>
            `;
            container.appendChild(card);
        }, index * 200);
    });
}

// Text-to-speech function
function speakChinese(text) {
    if ('speechSynthesis' in window) {
        // Cancel any ongoing speech
        window.speechSynthesis.cancel();
        
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = 'zh-CN';
        utterance.rate = 0.8;
        utterance.pitch = 1.0;
        utterance.volume = 1.0;
        
        // Get Chinese voice if available
        const voices = window.speechSynthesis.getVoices();
        const chineseVoice = voices.find(voice => 
            voice.lang === 'zh-CN' || 
            voice.lang.startsWith('zh')
        );
        
        if (chineseVoice) {
            utterance.voice = chineseVoice;
        }
        
        window.speechSynthesis.speak(utterance);
    } else {
        alert('Speech synthesis not supported in your browser. Try Chrome or Edge for audio playback!');
    }
}

// File upload handling
const uploadBox = document.getElementById('uploadBox');
const fileInput = document.getElementById('fileInput');
const previewSection = document.getElementById('previewSection');
const photoPreview = document.getElementById('photoPreview');
const generatingText = document.getElementById('generatingText');

uploadBox.addEventListener('click', (e) => {
    // Only trigger if the click is not on the file input itself
    if (e.target !== fileInput) {
        fileInput.click();
    }
});

// Drag and drop
uploadBox.addEventListener('dragover', (e) => {
    e.preventDefault();
    uploadBox.classList.add('dragover');
});

uploadBox.addEventListener('dragleave', () => {
    uploadBox.classList.remove('dragover');
});

uploadBox.addEventListener('drop', (e) => {
    e.preventDefault();
    uploadBox.classList.remove('dragover');
    
    const files = e.dataTransfer.files;
    if (files.length > 0 && files[0].type.startsWith('image/')) {
        handleImageUpload(files[0]);
    }
});

fileInput.addEventListener('change', (e) => {
    if (e.target.files.length > 0) {
        handleImageUpload(e.target.files[0]);
    }
});

async function handleImageUpload(file) {
    const reader = new FileReader();
    
    reader.onload = async (e) => {
        photoPreview.src = e.target.result;
        previewSection.classList.add('active');
        generatingText.classList.add('active');
        
        try {
            // AI analysis with loading indicator
            console.log('√∞≈∏¬ê¬± Starting image analysis for file:', file.name, 'Size:', file.size, 'Type:', file.type);
            const analysis = await analyzeImage(file);
            console.log('√∞≈∏‚Äù¬ç Image analysis result:', analysis);
            
            const captions = await generateCaptions(analysis);
            console.log('√∞≈∏‚Äú¬ù Generated captions:', captions);
            generatingText.classList.remove('active');
            displayCaptions(captions);
        } catch (error) {
            console.error('Error processing image:', error);
            generatingText.classList.remove('active');
            
            // Show detailed error message to user
            const container = document.getElementById('captionsContainer');
            let errorMessage = '√¢≈° √Ø¬∏¬è Sorry, there was an error analyzing your image.';
            let errorDetails = 'Please try again or check your internet connection.';
            // No fallback options - we want personalized results only
            
            // Provide more specific error information
            if (error.message.includes('API request failed')) {
                if (error.message.includes('402')) {
                    errorMessage = '√∞≈∏‚Äô¬∞ API Credits Insufficient';
                    errorDetails = 'Your OpenRouter account needs more credits. Please add credits at https://openrouter.ai/settings/credits or try again later.';
                } else if (error.message.includes('400')) {
                    errorMessage = '√∞≈∏‚Äù¬ß AI Model Unavailable';
                    errorDetails = 'The AI model is temporarily unavailable. Please try again later for personalized captions.';
                    // No fallback available
                } else {
                    errorMessage = '√∞≈∏≈í¬ê AI Service Unavailable';
                    errorDetails = 'The AI service is temporarily down. Please try again later for personalized captions.';
                    // No fallback available
                }
            } else if (error.message.includes('network') || error.message.includes('fetch')) {
                errorMessage = '√∞≈∏‚Äú¬° Network Connection Issue';
                errorDetails = 'Check your internet connection and try again to get personalized captions.';
            } else if (error.message.includes('vision model') || error.message.includes('personalized captions')) {
                errorMessage = '√∞≈∏‚Äò¬Å√Ø¬∏¬è AI Analysis Failed';
                errorDetails = 'Unable to analyze the image content for personalized captions. Please try again.';
                showFallback = true;
            }
            
            // No fallback captions - we want personalized captions only
            errorDetails = 'AI service is temporarily unavailable. Please try again later for personalized captions.';
            
            container.innerHTML = `
                <div class="error-message">
                    <h3>${errorMessage}</h3>
                    <p>${errorDetails}</p>
                    <details>
                        <summary>Technical Details</summary>
                        <p><small>Error: ${error.message}</small></p>
                    </details>
                    <button onclick="resetUpload()" class="retry-btn">Try Again</button>
                </div>
            `;
        }
    };
    
    reader.readAsDataURL(file);
}

function resetUpload() {
    previewSection.classList.remove('active');
    fileInput.value = '';
    document.getElementById('captionsContainer').innerHTML = '';
}

// Initialize speech synthesis voices
if ('speechSynthesis' in window) {
    // Load voices
    window.speechSynthesis.onvoiceschanged = () => {
        window.speechSynthesis.getVoices();
    };
    
    // Initial load
    window.speechSynthesis.getVoices();
}

// Initialize on page load
document.addEventListener('DOMContentLoaded', async () => {
    console.log('ü§î AI-Powered Mandarin Photo Captions loaded!');
    console.log('‚ú® Now with FULLY AI-GENERATED keywords and analysis!');
    console.log('√∞≈∏‚Äù¬ç No hardcoded keywords - everything is dynamically generated by AI!');
    console.log('√∞≈∏‚Äù¬ç Debug mode: Check console for detailed analysis logs');
    
    // Validate API keys on startup
    const keysValid = validateApiKeys();
    
    // Test API connectivity if keys are valid
    if (keysValid) {
        await testApiConnectivity();
    }
    
    // Add debug info to page
    const debugInfo = document.createElement('div');
    debugInfo.style.cssText = 'position: fixed; top: 10px; right: 10px; background: rgba(0,0,0,0.8); color: white; padding: 10px; border-radius: 5px; font-size: 12px; z-index: 1000;';
    debugInfo.innerHTML = '√∞≈∏‚Äù¬ç Debug Mode Active<br>Check Console for Details';
    document.body.appendChild(debugInfo);
    
    // Remove debug info after 5 seconds
    setTimeout(() => {
        if (debugInfo.parentNode) {
            debugInfo.parentNode.removeChild(debugInfo);
        }
    }, 5000);
});